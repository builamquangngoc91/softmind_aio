{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "afca4ef4-e178-49aa-9e81-e81675b5035f",
   "cell_type": "code",
   "source": "%%capture\nimport os\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n    !pip install --no-deps unsloth",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T14:46:52.795101Z",
     "iopub.execute_input": "2025-07-05T14:46:52.795772Z",
     "iopub.status.idle": "2025-07-05T14:47:05.780894Z",
     "shell.execute_reply.started": "2025-07-05T14:46:52.795745Z",
     "shell.execute_reply": "2025-07-05T14:47:05.780064Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "id": "a6ed97d5-94fa-4630-9e70-89a3f53732f3",
   "cell_type": "code",
   "source": "import torch\nprint(torch.__version__)\nprint(torch.cuda.is_available())\nprint(torch.cuda.get_device_name(0))\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T14:47:16.093488Z",
     "iopub.execute_input": "2025-07-05T14:47:16.094394Z",
     "iopub.status.idle": "2025-07-05T14:47:20.901153Z",
     "shell.execute_reply.started": "2025-07-05T14:47:16.094339Z",
     "shell.execute_reply": "2025-07-05T14:47:20.900524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "2.6.0+cu124\nTrue\nTesla P100-PCIE-16GB\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "id": "dbda2825-e429-48c2-8ed3-8926711230d2",
   "cell_type": "code",
   "source": "from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T14:47:43.774204Z",
     "iopub.execute_input": "2025-07-05T14:47:43.774754Z",
     "iopub.status.idle": "2025-07-05T14:48:35.168031Z",
     "shell.execute_reply.started": "2025-07-05T14:47:43.774723Z",
     "shell.execute_reply": "2025-07-05T14:48:35.167397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2025-07-05 14:47:56.249049: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751726876.489657      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751726876.562962      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "🦥 Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.6.12: Fast Qwen3 patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 6.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/1.41G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "152b30d9c9e448f39e9f3b18a7b11491"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06d906152c0e47e8a74c4601b3edf95f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e6f245037b24fa08ea742b72476c593"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "vocab.json: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc16e1fc12114f468231605f86958912"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "merges.txt: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a67811a7076c4a5da29eb8d3ccb0137d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9fc124436bc4853b1058bd7068d563d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ce9f79360a2402daa6f1468803811b5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f71a393226d841fdb215324bf863b11d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "chat_template.jinja: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f628cf6cb374f448470e588c393c8fe"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 3
  },
  {
   "id": "88061181-2f92-4c37-b9cb-ccbe66f7ae20",
   "cell_type": "code",
   "source": "model = FastLanguageModel.get_peft_model(\n    model,\n    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",\n\n                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n    lora_alpha = 32,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = True,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T14:48:39.655166Z",
     "iopub.execute_input": "2025-07-05T14:48:39.655970Z",
     "iopub.status.idle": "2025-07-05T14:48:51.504322Z",
     "shell.execute_reply.started": "2025-07-05T14:48:39.655939Z",
     "shell.execute_reply": "2025-07-05T14:48:51.503766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Unsloth: Offloading input_embeddings to disk to save VRAM\nUnsloth: Offloading output_embeddings to disk to save VRAM\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Unsloth 2025.6.12 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Unsloth: Training embed_tokens in mixed precision to save VRAM\nUnsloth: Training lm_head in mixed precision to save VRAM\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 4
  },
  {
   "id": "cab5430d-ff8d-4af3-8d3f-b30cc930dca1",
   "cell_type": "code",
   "source": "!gdown 1OnxJ_UeJ_YXRX0E1U7lBIxqI9phaI6wq",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T14:50:21.987640Z",
     "iopub.execute_input": "2025-07-05T14:50:21.987989Z",
     "iopub.status.idle": "2025-07-05T14:50:24.855462Z",
     "shell.execute_reply.started": "2025-07-05T14:50:21.987958Z",
     "shell.execute_reply": "2025-07-05T14:50:24.854719Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Downloading...\nFrom (original): https://drive.google.com/uc?id=1OnxJ_UeJ_YXRX0E1U7lBIxqI9phaI6wq\nFrom (redirected): https://drive.google.com/uc?id=1OnxJ_UeJ_YXRX0E1U7lBIxqI9phaI6wq&confirm=t&uuid=8f7625fd-2973-45b4-bfaf-c7f81afe8a7c\nTo: /kaggle/working/legal_corpus.json\n100%|████████████████████████████████████████| 117M/117M [00:01<00:00, 83.4MB/s]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 6
  },
  {
   "id": "5f45e919-b8b5-4b1b-9c26-0c1fc7d3d89f",
   "cell_type": "code",
   "source": "import json\n\nwith open(\"legal_corpus.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\nwith open(\"formatted_dataset.jsonl\", \"w\", encoding=\"utf-8\") as out_file:\n    for law in data:\n        law_id = law.get(\"law_id\", \"unknown\")\n        for article in law.get(\"content\", []):\n            article_text = article.get(\"content_Article\", \"\").strip()\n            if article_text:\n                item = {\"text\": f\"{law_id}: {article_text}\"}\n                json.dump(item, out_file, ensure_ascii=False)\n                out_file.write(\"\\n\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T14:55:02.900821Z",
     "iopub.execute_input": "2025-07-05T14:55:02.901743Z",
     "iopub.status.idle": "2025-07-05T14:55:06.229449Z",
     "shell.execute_reply.started": "2025-07-05T14:55:02.901710Z",
     "shell.execute_reply": "2025-07-05T14:55:06.228576Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "id": "39fa7dd8-6b86-4e17-902e-917363a82e1c",
   "cell_type": "code",
   "source": "with open(\"formatted_dataset.jsonl\", \"r\", encoding=\"utf-8\") as f:\n    for i in range(1):\n        print(f.readline())\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T14:55:25.880678Z",
     "iopub.execute_input": "2025-07-05T14:55:25.881580Z",
     "iopub.status.idle": "2025-07-05T14:55:25.886002Z",
     "shell.execute_reply.started": "2025-07-05T14:55:25.881553Z",
     "shell.execute_reply": "2025-07-05T14:55:25.885306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "{\"text\": \"14/2022/TT-NHNN: 1. Thông tư này quy định mã số, tiêu chuẩn chuyên môn, nghiệp vụ và xếp lương đối với các ngạch công chức chuyên ngành Ngân hàng.\\n\\n2. Thông tư này áp dụng đối với công chức làm việc tại các đơn vị thuộc Ngân hàng Nhà nước Việt Nam (gọi tắt là Ngân hàng Nhà nước).\"}\n\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 13
  },
  {
   "id": "c32bb3a7-b1a6-4077-a6f2-8e83af232e09",
   "cell_type": "code",
   "source": "from datasets import load_dataset\n\ndataset = load_dataset(\"json\", data_files=\"formatted_dataset.jsonl\", split=\"train\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T14:55:31.516683Z",
     "iopub.execute_input": "2025-07-05T14:55:31.517004Z",
     "iopub.status.idle": "2025-07-05T14:55:32.324144Z",
     "shell.execute_reply.started": "2025-07-05T14:55:31.516984Z",
     "shell.execute_reply": "2025-07-05T14:55:32.323498Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68edbb2f0e414094ae1bd34ba939e266"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 14
  },
  {
   "id": "a84e4e42-6424-4655-a8bd-e8cc6deab689",
   "cell_type": "code",
   "source": "from transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\nfrom unsloth import UnslothTrainer, UnslothTrainingArguments\n\ntrainer = UnslothTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n\n    args = UnslothTrainingArguments(\n        per_device_train_batch_size = 4,\n        gradient_accumulation_steps = 16,\n    \n        num_train_epochs = 1,\n        learning_rate = 2e-5,\n        embedding_learning_rate = 2e-6,\n        warmup_ratio = 0.1,\n        lr_scheduler_type = \"linear\",\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(), \n        #bf16 = True,\n        #fp16 = False,\n        logging_steps = 10,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T14:56:30.891200Z",
     "iopub.execute_input": "2025-07-05T14:56:30.891737Z",
     "iopub.status.idle": "2025-07-05T14:57:07.413899Z",
     "shell.execute_reply.started": "2025-07-05T14:56:30.891713Z",
     "shell.execute_reply": "2025-07-05T14:57:07.413093Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/59628 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e3baf51c7d84922b08b20e0f353097c"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 17
  },
  {
   "id": "69eb18e9-0360-480f-a7d7-8f7269821568",
   "cell_type": "code",
   "source": "trainer_stats = trainer.train()\nfrom huggingface_hub import login\nimport os\n\n# Set HF_TOKEN environment variable before running\nhf_token = os.getenv(\"HF_TOKEN\")\nif hf_token:\n    login(token=hf_token)\n    model.push_to_hub(\"thailevann/Qwen3-1.7B_CT_VLSP_track5\") \n    tokenizer.push_to_hub(\"thailevann/Qwen3-1.7B_CT_VLSP_track5\")\nelse:\n    print(\"Warning: HF_TOKEN environment variable not set. Skipping model upload.\")",
   "metadata": {},
   "outputs": []
  }
 ]
}