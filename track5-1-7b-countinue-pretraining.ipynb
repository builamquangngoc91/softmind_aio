{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "afca4ef4-e178-49aa-9e81-e81675b5035f",
   "cell_type": "code",
   "source": "%%capture\nimport os\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n    !pip install --no-deps unsloth",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T14:46:52.795101Z",
     "iopub.execute_input": "2025-07-05T14:46:52.795772Z",
     "iopub.status.idle": "2025-07-05T14:47:05.780894Z",
     "shell.execute_reply.started": "2025-07-05T14:46:52.795745Z",
     "shell.execute_reply": "2025-07-05T14:47:05.780064Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "id": "a6ed97d5-94fa-4630-9e70-89a3f53732f3",
   "cell_type": "code",
   "source": "import torch\nprint(torch.__version__)\nprint(torch.cuda.is_available())\nprint(torch.cuda.get_device_name(0))\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T14:47:16.093488Z",
     "iopub.execute_input": "2025-07-05T14:47:16.094394Z",
     "iopub.status.idle": "2025-07-05T14:47:20.901153Z",
     "shell.execute_reply.started": "2025-07-05T14:47:16.094339Z",
     "shell.execute_reply": "2025-07-05T14:47:20.900524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "2.6.0+cu124\nTrue\nTesla P100-PCIE-16GB\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "id": "dbda2825-e429-48c2-8ed3-8926711230d2",
   "cell_type": "code",
   "source": "from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T14:47:43.774204Z",
     "iopub.execute_input": "2025-07-05T14:47:43.774754Z",
     "iopub.status.idle": "2025-07-05T14:48:35.168031Z",
     "shell.execute_reply.started": "2025-07-05T14:47:43.774723Z",
     "shell.execute_reply": "2025-07-05T14:48:35.167397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "2025-07-05 14:47:56.249049: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751726876.489657      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751726876.562962      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.6.12: Fast Qwen3 patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 6.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/1.41G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "152b30d9c9e448f39e9f3b18a7b11491"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06d906152c0e47e8a74c4601b3edf95f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e6f245037b24fa08ea742b72476c593"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "vocab.json: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc16e1fc12114f468231605f86958912"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "merges.txt: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a67811a7076c4a5da29eb8d3ccb0137d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9fc124436bc4853b1058bd7068d563d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ce9f79360a2402daa6f1468803811b5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f71a393226d841fdb215324bf863b11d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "chat_template.jinja: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f628cf6cb374f448470e588c393c8fe"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 3
  },
  {
   "id": "88061181-2f92-4c37-b9cb-ccbe66f7ae20",
   "cell_type": "code",
   "source": "model = FastLanguageModel.get_peft_model(\n    model,\n    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",\n\n                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n    lora_alpha = 32,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = True,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T14:48:39.655166Z",
     "iopub.execute_input": "2025-07-05T14:48:39.655970Z",
     "iopub.status.idle": "2025-07-05T14:48:51.504322Z",
     "shell.execute_reply.started": "2025-07-05T14:48:39.655939Z",
     "shell.execute_reply": "2025-07-05T14:48:51.503766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Unsloth: Offloading input_embeddings to disk to save VRAM\nUnsloth: Offloading output_embeddings to disk to save VRAM\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Unsloth 2025.6.12 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Unsloth: Training embed_tokens in mixed precision to save VRAM\nUnsloth: Training lm_head in mixed precision to save VRAM\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 4
  },
  {
   "id": "cab5430d-ff8d-4af3-8d3f-b30cc930dca1",
   "cell_type": "code",
   "source": "!gdown 1OnxJ_UeJ_YXRX0E1U7lBIxqI9phaI6wq",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T14:50:21.987640Z",
     "iopub.execute_input": "2025-07-05T14:50:21.987989Z",
     "iopub.status.idle": "2025-07-05T14:50:24.855462Z",
     "shell.execute_reply.started": "2025-07-05T14:50:21.987958Z",
     "shell.execute_reply": "2025-07-05T14:50:24.854719Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Downloading...\nFrom (original): https://drive.google.com/uc?id=1OnxJ_UeJ_YXRX0E1U7lBIxqI9phaI6wq\nFrom (redirected): https://drive.google.com/uc?id=1OnxJ_UeJ_YXRX0E1U7lBIxqI9phaI6wq&confirm=t&uuid=8f7625fd-2973-45b4-bfaf-c7f81afe8a7c\nTo: /kaggle/working/legal_corpus.json\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 117M/117M [00:01<00:00, 83.4MB/s]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 6
  },
  {
   "id": "5f45e919-b8b5-4b1b-9c26-0c1fc7d3d89f",
   "cell_type": "code",
   "source": "import json\n\nwith open(\"legal_corpus.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\nwith open(\"formatted_dataset.jsonl\", \"w\", encoding=\"utf-8\") as out_file:\n    for law in data:\n        law_id = law.get(\"law_id\", \"unknown\")\n        for article in law.get(\"content\", []):\n            article_text = article.get(\"content_Article\", \"\").strip()\n            if article_text:\n                item = {\"text\": f\"{law_id}: {article_text}\"}\n                json.dump(item, out_file, ensure_ascii=False)\n                out_file.write(\"\\n\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T14:55:02.900821Z",
     "iopub.execute_input": "2025-07-05T14:55:02.901743Z",
     "iopub.status.idle": "2025-07-05T14:55:06.229449Z",
     "shell.execute_reply.started": "2025-07-05T14:55:02.901710Z",
     "shell.execute_reply": "2025-07-05T14:55:06.228576Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "id": "39fa7dd8-6b86-4e17-902e-917363a82e1c",
   "cell_type": "code",
   "source": "with open(\"formatted_dataset.jsonl\", \"r\", encoding=\"utf-8\") as f:\n    for i in range(1):\n        print(f.readline())\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T14:55:25.880678Z",
     "iopub.execute_input": "2025-07-05T14:55:25.881580Z",
     "iopub.status.idle": "2025-07-05T14:55:25.886002Z",
     "shell.execute_reply.started": "2025-07-05T14:55:25.881553Z",
     "shell.execute_reply": "2025-07-05T14:55:25.885306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "{\"text\": \"14/2022/TT-NHNN: 1. ThÃ´ng tÆ° nÃ y quy Ä‘á»‹nh mÃ£ sá»‘, tiÃªu chuáº©n chuyÃªn mÃ´n, nghiá»‡p vá»¥ vÃ  xáº¿p lÆ°Æ¡ng Ä‘á»‘i vá»›i cÃ¡c ngáº¡ch cÃ´ng chá»©c chuyÃªn ngÃ nh NgÃ¢n hÃ ng.\\n\\n2. ThÃ´ng tÆ° nÃ y Ã¡p dá»¥ng Ä‘á»‘i vá»›i cÃ´ng chá»©c lÃ m viá»‡c táº¡i cÃ¡c Ä‘Æ¡n vá»‹ thuá»™c NgÃ¢n hÃ ng NhÃ  nÆ°á»›c Viá»‡t Nam (gá»i táº¯t lÃ  NgÃ¢n hÃ ng NhÃ  nÆ°á»›c).\"}\n\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 13
  },
  {
   "id": "c32bb3a7-b1a6-4077-a6f2-8e83af232e09",
   "cell_type": "code",
   "source": "from datasets import load_dataset\n\ndataset = load_dataset(\"json\", data_files=\"formatted_dataset.jsonl\", split=\"train\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T14:55:31.516683Z",
     "iopub.execute_input": "2025-07-05T14:55:31.517004Z",
     "iopub.status.idle": "2025-07-05T14:55:32.324144Z",
     "shell.execute_reply.started": "2025-07-05T14:55:31.516984Z",
     "shell.execute_reply": "2025-07-05T14:55:32.323498Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68edbb2f0e414094ae1bd34ba939e266"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 14
  },
  {
   "id": "a84e4e42-6424-4655-a8bd-e8cc6deab689",
   "cell_type": "code",
   "source": "from transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\nfrom unsloth import UnslothTrainer, UnslothTrainingArguments\n\ntrainer = UnslothTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n\n    args = UnslothTrainingArguments(\n        per_device_train_batch_size = 4,\n        gradient_accumulation_steps = 16,\n    \n        num_train_epochs = 1,\n        learning_rate = 2e-5,\n        embedding_learning_rate = 2e-6,\n        warmup_ratio = 0.1,\n        lr_scheduler_type = \"linear\",\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(), \n        #bf16 = True,\n        #fp16 = False,\n        logging_steps = 10,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-07-05T14:56:30.891200Z",
     "iopub.execute_input": "2025-07-05T14:56:30.891737Z",
     "iopub.status.idle": "2025-07-05T14:57:07.413899Z",
     "shell.execute_reply.started": "2025-07-05T14:56:30.891713Z",
     "shell.execute_reply": "2025-07-05T14:57:07.413093Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/59628 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e3baf51c7d84922b08b20e0f353097c"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 17
  },
  {
   "id": "69eb18e9-0360-480f-a7d7-8f7269821568",
   "cell_type": "code",
   "source": "trainer_stats = trainer.train()\nfrom huggingface_hub import login\nimport os\n\n# Set HF_TOKEN environment variable before running\nhf_token = os.getenv(\"HF_TOKEN\")\nif hf_token:\n    login(token=hf_token)\n    model.push_to_hub(\"thailevann/Qwen3-1.7B_CT_VLSP_track5\") \n    tokenizer.push_to_hub(\"thailevann/Qwen3-1.7B_CT_VLSP_track5\")\nelse:\n    print(\"Warning: HF_TOKEN environment variable not set. Skipping model upload.\")",
   "metadata": {},
   "outputs": []
  }
 ]
}